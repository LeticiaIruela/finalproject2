{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2379c8ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.preprocessing import image\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aff67238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input, EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b2c4b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b5fa099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35aac772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>battery/battery1.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>battery/battery10.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>battery/battery100.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>battery/battery101.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>battery/battery102.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15510</th>\n",
       "      <td>white-glass/white-glass95.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15511</th>\n",
       "      <td>white-glass/white-glass96.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15512</th>\n",
       "      <td>white-glass/white-glass97.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15513</th>\n",
       "      <td>white-glass/white-glass98.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15514</th>\n",
       "      <td>white-glass/white-glass99.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15515 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Image  Category\n",
       "0               battery/battery1.jpg         0\n",
       "1              battery/battery10.jpg         0\n",
       "2             battery/battery100.jpg         0\n",
       "3             battery/battery101.jpg         0\n",
       "4             battery/battery102.jpg         0\n",
       "...                              ...       ...\n",
       "15510  white-glass/white-glass95.jpg        12\n",
       "15511  white-glass/white-glass96.jpg        12\n",
       "15512  white-glass/white-glass97.jpg        12\n",
       "15513  white-glass/white-glass98.jpg        12\n",
       "15514  white-glass/white-glass99.jpg        12\n",
       "\n",
       "[15515 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('df_f.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f9d2c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "garbage_types_labels = {\n",
    "    0: 'battery',\n",
    "    1: 'biological',\n",
    "    3: 'brown-glass',\n",
    "    4: 'cardboard',\n",
    "    5: 'clothes',\n",
    "    6: 'green-glass',\n",
    "    7: 'metal',\n",
    "    8: 'paper',\n",
    "    9: 'plastic',\n",
    "    10: 'shoes',\n",
    "    11: 'trash',\n",
    "    12: 'white-glass'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97eb13c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>battery/battery1.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>battery/battery10.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>battery/battery100.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>battery/battery101.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>battery/battery102.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15510</th>\n",
       "      <td>white-glass/white-glass95.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15511</th>\n",
       "      <td>white-glass/white-glass96.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15512</th>\n",
       "      <td>white-glass/white-glass97.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15513</th>\n",
       "      <td>white-glass/white-glass98.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15514</th>\n",
       "      <td>white-glass/white-glass99.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15515 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Image  Category\n",
       "0               battery/battery1.jpg         0\n",
       "1              battery/battery10.jpg         0\n",
       "2             battery/battery100.jpg         0\n",
       "3             battery/battery101.jpg         0\n",
       "4             battery/battery102.jpg         0\n",
       "...                              ...       ...\n",
       "15510  white-glass/white-glass95.jpg        12\n",
       "15511  white-glass/white-glass96.jpg        12\n",
       "15512  white-glass/white-glass97.jpg        12\n",
       "15513  white-glass/white-glass98.jpg        12\n",
       "15514  white-glass/white-glass99.jpg        12\n",
       "\n",
       "[15515 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d76de3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Category\"] = df[\"Category\"].replace(garbage_types_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5145134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>battery/battery1.jpg</td>\n",
       "      <td>battery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>battery/battery10.jpg</td>\n",
       "      <td>battery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>battery/battery100.jpg</td>\n",
       "      <td>battery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>battery/battery101.jpg</td>\n",
       "      <td>battery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>battery/battery102.jpg</td>\n",
       "      <td>battery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15510</th>\n",
       "      <td>white-glass/white-glass95.jpg</td>\n",
       "      <td>white-glass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15511</th>\n",
       "      <td>white-glass/white-glass96.jpg</td>\n",
       "      <td>white-glass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15512</th>\n",
       "      <td>white-glass/white-glass97.jpg</td>\n",
       "      <td>white-glass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15513</th>\n",
       "      <td>white-glass/white-glass98.jpg</td>\n",
       "      <td>white-glass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15514</th>\n",
       "      <td>white-glass/white-glass99.jpg</td>\n",
       "      <td>white-glass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15515 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Image     Category\n",
       "0               battery/battery1.jpg      battery\n",
       "1              battery/battery10.jpg      battery\n",
       "2             battery/battery100.jpg      battery\n",
       "3             battery/battery101.jpg      battery\n",
       "4             battery/battery102.jpg      battery\n",
       "...                              ...          ...\n",
       "15510  white-glass/white-glass95.jpg  white-glass\n",
       "15511  white-glass/white-glass96.jpg  white-glass\n",
       "15512  white-glass/white-glass97.jpg  white-glass\n",
       "15513  white-glass/white-glass98.jpg  white-glass\n",
       "15514  white-glass/white-glass99.jpg  white-glass\n",
       "\n",
       "[15515 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6959b803",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validate_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "validate_df, test_df = train_test_split(validate_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "validate_df = validate_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "train = train_df.shape[0]\n",
    "validate = validate_df.shape[0]\n",
    "test=test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "056f2516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1552"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3eed09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1551"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0dfff79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12412"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d985c569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "clothes        4238\n",
       "shoes          1596\n",
       "paper           844\n",
       "biological      804\n",
       "battery         751\n",
       "cardboard       717\n",
       "plastic         685\n",
       "white-glass     630\n",
       "metal           601\n",
       "trash           571\n",
       "green-glass     498\n",
       "brown-glass     477\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba92f0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "clothes        548\n",
       "shoes          200\n",
       "biological     105\n",
       "plastic         92\n",
       "metal           89\n",
       "battery         87\n",
       "paper           86\n",
       "white-glass     77\n",
       "cardboard       76\n",
       "brown-glass     67\n",
       "green-glass     63\n",
       "trash           61\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da3f707d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "clothes        539\n",
       "shoes          181\n",
       "paper          120\n",
       "battery        107\n",
       "cardboard       98\n",
       "plastic         88\n",
       "metal           79\n",
       "biological      76\n",
       "white-glass     68\n",
       "green-glass     68\n",
       "trash           65\n",
       "brown-glass     63\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeaeb16",
   "metadata": {},
   "source": [
    "### TO CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47af9c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train.csv', index=False)\n",
    "test_df.to_csv('test.csv',index=False)\n",
    "validate_df.to_csv('validation.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cce2079",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6550cb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 224\n",
    "img_width = 224\n",
    "img_size=(img_width,img_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5bdc950",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, validation_split=0.2)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65dff750",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"garbage_classification/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95c45057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12412 validated image filenames belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df, \n",
    "    directory=data_directory, \n",
    "    x_col='Image',\n",
    "    y_col='Category',\n",
    "    target_size=img_size,\n",
    "    class_mode='categorical',\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec149eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1551 validated image filenames belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=validate_df,\n",
    "    directory=data_directory,\n",
    "    x_col='Image',\n",
    "    y_col='Category',\n",
    "    target_size=img_size,\n",
    "    class_mode='categorical',\n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0c1b3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1552 validated image filenames belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    directory=data_directory,\n",
    "    x_col='Image',\n",
    "    y_col='Category',\n",
    "    target_size=img_size,\n",
    "    class_mode='categorical',\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d895f565",
   "metadata": {},
   "source": [
    "# 1. model_0306_efficientnetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8176d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "#model_0306_efficientnetB0\n",
    "\n",
    "def create_model(path, name, img_height, img_width, train_generator, validation_generator, epochs):\n",
    "    num_classes = 12\n",
    "\n",
    "    start = datetime.now()\n",
    "\n",
    "    # Crear el modelo base\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Definir el modelo final\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Congelar las capas del modelo base\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    history=model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
    "\n",
    "    try:\n",
    "        model.save(f\"{path}/{name}.h5\")\n",
    "    except:\n",
    "        print(\"Modelo no guardado\")\n",
    "\n",
    "    end = datetime.now()\n",
    "\n",
    "    print(f\"It took {end-start} time\")\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b733a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45/388 [==>...........................] - ETA: 7:42 - loss: 0.6474 - accuracy: 0.8090"
     ]
    }
   ],
   "source": [
    "history=create_model(\".\", \"model_0306_efficientnetB0\", 224, 224, train_generator, validation_generator, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a4dc39a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      4\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGiCAYAAADA0E3hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdU0lEQVR4nO3df2zddb348Vfb0VOItMy7u+6HxWUYRAU23VgtSAimugQz7v4w7oLZ5sKPi+4SXaOyOVhFdJ2IuASGixPEP/RuSsAYtwy1shhkZsm2JngZEBi4aWxh0bVzSMva9/3DL/XbrYWd7vTHe3s8kvNHP3w+Pa8C55Vnz2l7ylJKKQAAMlA+1gMAAJws4QIAZEO4AADZEC4AQDaECwCQDeECAGRDuAAA2RAuAEA2hAsAkA3hAgBko+hw+e1vfxsLFiyIadOmRVlZWfzsZz9722t27NgRH/rQh6JQKMR73vOeePjhh4cxKpArewMolaLD5ejRozFr1qzYsGHDSZ3/0ksvxSc+8Ym4+uqro62tLb7whS/EjTfeGI8//njRwwJ5sjeAUik7lTdZLCsri8ceeywWLlw45Dm33XZbbN26Nf7whz/0H/vP//zPOHz4cGzfvn24dw1kyt4ATsWEkb6DnTt3RmNj44Bj8+fPjy984QtDXtPd3R3d3d39H/f19cVf//rX+Ld/+7coKysbqVGBIaSU4siRIzFt2rQoLx/5H42zN+D0MBK7Y8TDpb29PWprawccq62tja6urvjHP/4RZ5999gnXtLS0xJ133jnSowFFOnjwYLzrXe8a8fuxN+D0UsrdMeLhMhyrVq2Kpqam/o87Ozvj/PPPj4MHD0Z1dfUYTgZnpq6urqirq4tzzz13rEcZkr0B489I7I4RD5cpU6ZER0fHgGMdHR1RXV096HdNERGFQiEKhcIJx6urqy0gGEOj9ZKLvQGnl1LujhF/sbqhoSFaW1sHHPvVr34VDQ0NI33XQKbsDWAoRYfL3//+92hra4u2traI+OevLba1tcWBAwci4p9P1y5ZsqT//FtuuSX2798fX/7yl+PZZ5+NBx54IH7yk5/EihUrSvMVAOOevQGUTCrSE088kSLihNvSpUtTSiktXbo0XXXVVSdcM3v27FRZWZlmzpyZfvCDHxR1n52dnSkiUmdnZ7HjAiVwqo9BewPOTCPxODylv+MyWrq6uqKmpiY6Ozu9Vg1jIMfHYI4zw+lmJB6H3qsIAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsDCtcNmzYEDNmzIiqqqqor6+PXbt2veX569evj/e+971x9tlnR11dXaxYsSJef/31YQ0M5MneAEqh6HDZsmVLNDU1RXNzc+zZsydmzZoV8+fPj1deeWXQ83/84x/HypUro7m5Ofbt2xcPPvhgbNmyJb7yla+c8vBAHuwNoFSKDpd77703brrppli2bFm8//3vj40bN8Y555wTDz300KDnP/XUU3HFFVfE9ddfHzNmzIiPf/zjcd11173td1vA6cPeAEqlqHDp6emJ3bt3R2Nj478+QXl5NDY2xs6dOwe95vLLL4/du3f3L5z9+/fHtm3b4pprrhnyfrq7u6Orq2vADciTvQGU0oRiTj506FD09vZGbW3tgOO1tbXx7LPPDnrN9ddfH4cOHYqPfOQjkVKKY8eOxS233PKWT/m2tLTEnXfeWcxowDhlbwClNOK/VbRjx45Yu3ZtPPDAA7Fnz5549NFHY+vWrXHXXXcNec2qVauis7Oz/3bw4MGRHhMYR+wNYChFPeMyadKkqKioiI6OjgHHOzo6YsqUKYNec8cdd8TixYvjxhtvjIiISy65JI4ePRo333xzrF69OsrLT2ynQqEQhUKhmNGAccreAEqpqGdcKisrY86cOdHa2tp/rK+vL1pbW6OhoWHQa1577bUTlkxFRUVERKSUip0XyIy9AZRSUc+4REQ0NTXF0qVLY+7cuTFv3rxYv359HD16NJYtWxYREUuWLInp06dHS0tLREQsWLAg7r333vjgBz8Y9fX18cILL8Qdd9wRCxYs6F9EwOnN3gBKpehwWbRoUbz66quxZs2aaG9vj9mzZ8f27dv7f/DuwIEDA75Tuv3226OsrCxuv/32+POf/xz//u//HgsWLIhvfOMbpfsqgHHN3gBKpSxl8LxrV1dX1NTURGdnZ1RXV4/1OHDGyfExmOPMcLoZiceh9yoCALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwwqXDRs2xIwZM6Kqqirq6+tj165db3n+4cOHY/ny5TF16tQoFApx4YUXxrZt24Y1MJAnewMohQnFXrBly5ZoamqKjRs3Rn19faxfvz7mz58fzz33XEyePPmE83t6euJjH/tYTJ48OR555JGYPn16/PGPf4zzzjuvFPMDGbA3gFIpSymlYi6or6+Pyy67LO6///6IiOjr64u6urq49dZbY+XKlSecv3HjxvjWt74Vzz77bJx11lnDGrKrqytqamqis7Mzqqurh/U5gOE71cegvQFnppF4HBb1UlFPT0/s3r07Ghsb//UJysujsbExdu7cOeg1P//5z6OhoSGWL18etbW1cfHFF8fatWujt7d3yPvp7u6Orq6uATcgT/YGUEpFhcuhQ4eit7c3amtrBxyvra2N9vb2Qa/Zv39/PPLII9Hb2xvbtm2LO+64I7797W/H17/+9SHvp6WlJWpqavpvdXV1xYwJjCP2BlBKI/5bRX19fTF58uT43ve+F3PmzIlFixbF6tWrY+PGjUNes2rVqujs7Oy/HTx4cKTHBMYRewMYSlE/nDtp0qSoqKiIjo6OAcc7OjpiypQpg14zderUOOuss6KioqL/2Pve975ob2+Pnp6eqKysPOGaQqEQhUKhmNGAccreAEqpqGdcKisrY86cOdHa2tp/rK+vL1pbW6OhoWHQa6644op44YUXoq+vr//Y888/H1OnTh10+QCnF3sDKKWiXypqamqKTZs2xQ9/+MPYt29ffPazn42jR4/GsmXLIiJiyZIlsWrVqv7zP/vZz8Zf//rX+PznPx/PP/98bN26NdauXRvLly8v3VcBjGv2BlAqRf8dl0WLFsWrr74aa9asifb29pg9e3Zs3769/wfvDhw4EOXl/+qhurq6ePzxx2PFihVx6aWXxvTp0+Pzn/983HbbbaX7KoBxzd4ASqXov+MyFvw9BhhbOT4Gc5wZTjdj/ndcAADGknABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAwrXDZs2BAzZsyIqqqqqK+vj127dp3UdZs3b46ysrJYuHDhcO4WyJzdAZyqosNly5Yt0dTUFM3NzbFnz56YNWtWzJ8/P1555ZW3vO7ll1+OL37xi3HllVcOe1ggX3YHUApFh8u9994bN910Uyxbtize//73x8aNG+Occ86Jhx56aMhrent749Of/nTceeedMXPmzLe9j+7u7ujq6hpwA/I20rvD3oAzQ1Hh0tPTE7t3747GxsZ/fYLy8mhsbIydO3cOed3Xvva1mDx5ctxwww0ndT8tLS1RU1PTf6urqytmTGCcGY3dYW/AmaGocDl06FD09vZGbW3tgOO1tbXR3t4+6DVPPvlkPPjgg7Fp06aTvp9Vq1ZFZ2dn/+3gwYPFjAmMM6OxO+wNODNMGMlPfuTIkVi8eHFs2rQpJk2adNLXFQqFKBQKIzgZMJ4NZ3fYG3BmKCpcJk2aFBUVFdHR0THgeEdHR0yZMuWE81988cV4+eWXY8GCBf3H+vr6/nnHEybEc889FxdccMFw5gYyYncApVLUS0WVlZUxZ86caG1t7T/W19cXra2t0dDQcML5F110UTz99NPR1tbWf7v22mvj6quvjra2Nq9BwxnC7gBKpeiXipqammLp0qUxd+7cmDdvXqxfvz6OHj0ay5Yti4iIJUuWxPTp06OlpSWqqqri4osvHnD9eeedFxFxwnHg9GZ3AKVQdLgsWrQoXn311VizZk20t7fH7NmzY/v27f0/dHfgwIEoL/cHeYGB7A6gFMpSSmmsh3g7XV1dUVNTE52dnVFdXT3W48AZJ8fHYI4zw+lmJB6Hvr0BALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwwqXDRs2xIwZM6Kqqirq6+tj165dQ567adOmuPLKK2PixIkxceLEaGxsfMvzgdOX3QGcqqLDZcuWLdHU1BTNzc2xZ8+emDVrVsyfPz9eeeWVQc/fsWNHXHfddfHEE0/Ezp07o66uLj7+8Y/Hn//851MeHsiH3QGUQllKKRVzQX19fVx22WVx//33R0REX19f1NXVxa233horV6582+t7e3tj4sSJcf/998eSJUsGPae7uzu6u7v7P+7q6oq6urro7OyM6urqYsYFSqCrqytqampO6TE40rvD3oDxpxS743hFPePS09MTu3fvjsbGxn99gvLyaGxsjJ07d57U53jttdfijTfeiHe+851DntPS0hI1NTX9t7q6umLGBMaZ0dgd9gacGYoKl0OHDkVvb2/U1tYOOF5bWxvt7e0n9Tluu+22mDZt2oAFdrxVq1ZFZ2dn/+3gwYPFjAmMM6OxO+wNODNMGM07W7duXWzevDl27NgRVVVVQ55XKBSiUCiM4mTAeHYyu8PegDNDUeEyadKkqKioiI6OjgHHOzo6YsqUKW957T333BPr1q2LX//613HppZcWPymQLbsDKJWiXiqqrKyMOXPmRGtra/+xvr6+aG1tjYaGhiGvu/vuu+Ouu+6K7du3x9y5c4c/LZAluwMolaJfKmpqaoqlS5fG3LlzY968ebF+/fo4evRoLFu2LCIilixZEtOnT4+WlpaIiPjmN78Za9asiR//+McxY8aM/tez3/GOd8Q73vGOEn4pwHhmdwClUHS4LFq0KF599dVYs2ZNtLe3x+zZs2P79u39P3R34MCBKC//1xM53/3ud6Onpyc++clPDvg8zc3N8dWvfvXUpgeyYXcApVD033EZCyPxe+DAycvxMZjjzHC6GfO/4wIAMJaECwCQDeECAGRDuAAA2RAuAEA2hAsAkA3hAgBkQ7gAANkQLgBANoQLAJAN4QIAZEO4AADZEC4AQDaECwCQDeECAGRDuAAA2RAuAEA2hAsAkA3hAgBkQ7gAANkQLgBANoQLAJAN4QIAZEO4AADZEC4AQDaECwCQDeECAGRDuAAA2RAuAEA2hAsAkA3hAgBkQ7gAANkQLgBANoQLAJAN4QIAZEO4AADZEC4AQDaECwCQDeECAGRDuAAA2RAuAEA2hAsAkA3hAgBkQ7gAANkQLgBANoQLAJAN4QIAZEO4AADZEC4AQDaECwCQDeECAGRDuAAA2RAuAEA2hAsAkA3hAgBkQ7gAANkQLgBANoQLAJAN4QIAZEO4AADZEC4AQDaECwCQDeECAGRjWOGyYcOGmDFjRlRVVUV9fX3s2rXrLc//6U9/GhdddFFUVVXFJZdcEtu2bRvWsEDe7A7gVBUdLlu2bImmpqZobm6OPXv2xKxZs2L+/PnxyiuvDHr+U089Fdddd13ccMMNsXfv3li4cGEsXLgw/vCHP5zy8EA+7A6gFMpSSqmYC+rr6+Oyyy6L+++/PyIi+vr6oq6uLm699dZYuXLlCecvWrQojh49Gr/4xS/6j334wx+O2bNnx8aNGwe9j+7u7uju7u7/uLOzM84///w4ePBgVFdXFzMuUAJdXV1RV1cXhw8fjpqammF9jpHeHfYGjD+l2B0nSEXo7u5OFRUV6bHHHhtwfMmSJenaa68d9Jq6urr0ne98Z8CxNWvWpEsvvXTI+2lubk4R4ebmNs5uL774YjErY1R3h73h5jZ+b8PdHYOZEEU4dOhQ9Pb2Rm1t7YDjtbW18eyzzw56TXt7+6Dnt7e3D3k/q1atiqampv6PDx8+HO9+97vjwIEDpSu2EfZmZeb03Z6ZR0eOM7/57MU73/nOYV0/GrvD3hgbOc4ckefcOc58qrtjMEWFy2gpFApRKBROOF5TU5PNf6w3VVdXm3kUmHl0lJeP319EtDfGVo4zR+Q5d44zl3J3FPWZJk2aFBUVFdHR0THgeEdHR0yZMmXQa6ZMmVLU+cDpx+4ASqWocKmsrIw5c+ZEa2tr/7G+vr5obW2NhoaGQa9paGgYcH5ExK9+9ashzwdOP3YHUDLF/lDM5s2bU6FQSA8//HB65pln0s0335zOO++81N7enlJKafHixWnlypX95//ud79LEyZMSPfcc0/at29fam5uTmeddVZ6+umnT/o+X3/99dTc3Jxef/31YscdM2YeHWYeHaWYebR3x5n673m05ThzSnnObeZ/KjpcUkrpvvvuS+eff36qrKxM8+bNS7///e/7/9lVV12Vli5dOuD8n/zkJ+nCCy9MlZWV6QMf+EDaunXrKQ0N5MnuAE5V0X/HBQBgrIzfXxEAADiOcAEAsiFcAIBsCBcAIBvjJlxyfLv7YmbetGlTXHnllTFx4sSYOHFiNDY2vu3XOBKK/ff8ps2bN0dZWVksXLhwZAccRLEzHz58OJYvXx5Tp06NQqEQF1544aj//1HszOvXr4/3vve9cfbZZ0ddXV2sWLEiXn/99VGaNuK3v/1tLFiwIKZNmxZlZWXxs5/97G2v2bFjR3zoQx+KQqEQ73nPe+Lhhx8e8TmPZ2+MDntj9OS0O8Zsb4z1rzWl9M+/71BZWZkeeuih9L//+7/ppptuSuedd17q6OgY9Pzf/e53qaKiIt19993pmWeeSbfffnvRfxtmtGe+/vrr04YNG9LevXvTvn370mc+85lUU1OT/vSnP43bmd/00ksvpenTp6crr7wy/cd//MfoDPv/FDtzd3d3mjt3brrmmmvSk08+mV566aW0Y8eO1NbWNm5n/tGPfpQKhUL60Y9+lF566aX0+OOPp6lTp6YVK1aM2szbtm1Lq1evTo8++miKiBPeDPF4+/fvT+ecc05qampKzzzzTLrvvvtSRUVF2r59++gMnOyN8Trzm+yNkZ97rHfHWO2NcREu8+bNS8uXL+//uLe3N02bNi21tLQMev6nPvWp9IlPfGLAsfr6+vRf//VfIzrn/6/YmY937NixdO6556Yf/vCHIzXiCYYz87Fjx9Lll1+evv/976elS5eO+gIqdubvfve7aebMmamnp2e0RjxBsTMvX748ffSjHx1wrKmpKV1xxRUjOudQTmYBffnLX04f+MAHBhxbtGhRmj9//ghONpC9MTrsjdGT8+4Yzb0x5i8V9fT0xO7du6OxsbH/WHl5eTQ2NsbOnTsHvWbnzp0Dzo+ImD9//pDnl9pwZj7ea6+9Fm+88UZJ3zHzrQx35q997WsxefLkuOGGG0ZjzAGGM/PPf/7zaGhoiOXLl0dtbW1cfPHFsXbt2ujt7R23M19++eWxe/fu/qeE9+/fH9u2bYtrrrlmVGYejhwfgznOfDx74+3luDcizozdUarH4Ji/O/RovN19qQ1n5uPddtttMW3atBP+I46U4cz85JNPxoMPPhhtbW2jMOGJhjPz/v374ze/+U18+tOfjm3btsULL7wQn/vc5+KNN96I5ubmcTnz9ddfH4cOHYqPfOQjkVKKY8eOxS233BJf+cpXRnze4RrqMdjV1RX/+Mc/4uyzzx7R+7c37I2h5Lg3Is6M3VGqvTHmz7icidatWxebN2+Oxx57LKqqqsZ6nEEdOXIkFi9eHJs2bYpJkyaN9Tgnra+vLyZPnhzf+973Ys6cObFo0aJYvXp1bNy4caxHG9KOHTti7dq18cADD8SePXvi0Ucfja1bt8Zdd9011qMxjtgbIyfHvRFx5u6OMX/GJce3ux/OzG+65557Yt26dfHrX/86Lr300pEcc4BiZ37xxRfj5ZdfjgULFvQf6+vri4iICRMmxHPPPRcXXHDBuJo5ImLq1Klx1llnRUVFRf+x973vfdHe3h49PT1RWVk57ma+4447YvHixXHjjTdGRMQll1wSR48ejZtvvjlWr14d5eXj7/uLoR6D1dXVI/5sS4S9MVrsjdHZGxFnxu4o1d4Y868qx7e7H87MERF333133HXXXbF9+/aYO3fuaIzar9iZL7roonj66aejra2t/3bttdfG1VdfHW1tbVFXVzfuZo6IuOKKK+KFF17oX5YREc8//3xMnTp1VJbPcGZ+7bXXTlgwby7QNE7fSizHx2COM0fYGyM9c8TY742IM2N3lOwxWNSP8o6Q0X67+7GYed26damysjI98sgj6S9/+Uv/7ciRI+N25uONxW8HFDvzgQMH0rnnnpv++7//Oz333HPpF7/4RZo8eXL6+te/Pm5nbm5uTueee276n//5n7R///70y1/+Ml1wwQXpU5/61KjNfOTIkbR37960d+/eFBHp3nvvTXv37k1//OMfU0oprVy5Mi1evLj//Dd/rfFLX/pS2rdvX9qwYcOY/Dq0vTH+Zj6evTFyc4/17hirvTEuwiWlPN/uvpiZ3/3ud6eIOOHW3Nw8bmc+3lgsoJSKn/mpp55K9fX1qVAopJkzZ6ZvfOMb6dixY+N25jfeeCN99atfTRdccEGqqqpKdXV16XOf+1z629/+NmrzPvHEE4P+//nmnEuXLk1XXXXVCdfMnj07VVZWppkzZ6Yf/OAHozbvm+yN8Tfz8eyN4uS0O8Zqb5SlNA6fTwIAGMSY/4wLAMDJEi4AQDaECwCQDeECAGRDuAAA2RAuAEA2hAsAkA3hAgBkQ7gAANkQLgBANoQLAJCN/wM67QKIu94xvgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = list(range(1, 10 + 1))\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "train_acc = history['accuracy']\n",
    "train_loss = history['loss']\n",
    "val_acc = history['val_accuracy']\n",
    "val_loss = history['val_loss']\n",
    "\n",
    "fig.set_size_inches(20, 10)\n",
    "\n",
    "ax[0].plot(epochs, train_acc, 'go-', color='r', label='Training Accuracy')\n",
    "ax[0].plot(epochs, val_acc, 'go-', label='Validation Accuracy')\n",
    "ax[0].set_title('Training Accuracy')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "ax[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "ax[1].plot(epochs, train_loss, 'g-o', color='r', label='Training Loss')\n",
    "ax[1].plot(epochs, val_loss, 'go-', label='Validation Loss')\n",
    "ax[1].set_title('Training Loss')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "ax[1].set_ylabel(\"Training Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b91ccab",
   "metadata": {},
   "source": [
    "# 1.1 Retrain model_0306_efficientnetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e444777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from datetime import datetime\n",
    "\n",
    "def retrain_model(model_path, img_height, img_width, train_generator, validation_generator, epochs, save_path, save_name):\n",
    "    start = datetime.now()\n",
    "\n",
    "    # Cargar el modelo desde el archivo guardado\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    history=model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
    "\n",
    "    # Guardar el modelo una vez finalizado el entrenamiento\n",
    "    try:\n",
    "        model.save(f\"{save_path}/{save_name}.h5\")\n",
    "        print(\"Modelo guardado correctamente.\")\n",
    "    except:\n",
    "        print(\"Error al guardar el modelo.\")\n",
    "\n",
    "    end = datetime.now()\n",
    "    print(f\"Tiempo de entrenamiento: {end-start}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd4d4555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "388/388 [==============================] - 590s 2s/step - loss: 0.0886 - accuracy: 0.9704 - val_loss: 0.1557 - val_accuracy: 0.9516\n",
      "Epoch 2/10\n",
      "388/388 [==============================] - 619s 2s/step - loss: 0.0447 - accuracy: 0.9850 - val_loss: 0.1527 - val_accuracy: 0.9613\n",
      "Epoch 3/10\n",
      "388/388 [==============================] - 580s 1s/step - loss: 0.0241 - accuracy: 0.9927 - val_loss: 0.1525 - val_accuracy: 0.9632\n",
      "Epoch 4/10\n",
      "388/388 [==============================] - 577s 1s/step - loss: 0.0155 - accuracy: 0.9952 - val_loss: 0.1848 - val_accuracy: 0.9542\n",
      "Epoch 5/10\n",
      "388/388 [==============================] - 606s 2s/step - loss: 0.0196 - accuracy: 0.9930 - val_loss: 0.1913 - val_accuracy: 0.9587\n",
      "Epoch 6/10\n",
      "388/388 [==============================] - 603s 2s/step - loss: 0.0193 - accuracy: 0.9929 - val_loss: 0.1916 - val_accuracy: 0.9613\n",
      "Epoch 7/10\n",
      "388/388 [==============================] - 588s 2s/step - loss: 0.0174 - accuracy: 0.9942 - val_loss: 0.1826 - val_accuracy: 0.9562\n",
      "Epoch 8/10\n",
      "388/388 [==============================] - 625s 2s/step - loss: 0.0103 - accuracy: 0.9961 - val_loss: 0.1967 - val_accuracy: 0.9594\n",
      "Epoch 9/10\n",
      "388/388 [==============================] - 585s 2s/step - loss: 0.0068 - accuracy: 0.9982 - val_loss: 0.2221 - val_accuracy: 0.9549\n",
      "Epoch 10/10\n",
      "388/388 [==============================] - 585s 2s/step - loss: 0.0243 - accuracy: 0.9919 - val_loss: 0.2505 - val_accuracy: 0.9504\n",
      "Modelo guardado correctamente.\n",
      "Tiempo de entrenamiento: 1:39:19.555326\n"
     ]
    }
   ],
   "source": [
    "retrain_model(\"model_0306_efficientnetB0.h5\", 224, 224, train_generator, validation_generator, epochs=10, save_path=\".\", save_name=\"model_0306_efficientnetB0_retrain1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d15ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(len(history.history['accuracy'])))\n",
    "fig , ax = plt.subplots(1,2)\n",
    "train_acc = history.history['accuracy']\n",
    "train_loss = history.history['loss']\n",
    "val_acc = history.history['val_accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "fig.set_size_inches(20,10)\n",
    "\n",
    "ax[0].plot(epochs , train_acc , 'go-' ,color='r', label = 'Training Accuracy')\n",
    "ax[0].plot(epochs , val_acc , 'go-' , label = 'Validation Accuracy')\n",
    "ax[0].set_title('Training Accuracy')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "ax[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "\n",
    "ax[1].plot(epochs , train_loss , 'g-o' ,color='r', label = 'Training Loss')\n",
    "ax[1].plot(epochs , val_loss , 'go-' , label = 'Validation Loss')\n",
    "ax[1].set_title('Training Loss')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "ax[1].set_ylabel(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d402f1",
   "metadata": {},
   "source": [
    "# 2. model_0306_efficientnetB0 V2 + FINE+TUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6fd6600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#model_0306_efficientnetB0.v2\n",
    "\n",
    "def create_model(path, name, img_height, img_width, train_generator, validation_generator, epochs, fine_tuning=False, data_augmentation=False):\n",
    "    num_classes = 12\n",
    "\n",
    "    start = datetime.now()\n",
    "\n",
    "    # Crear el modelo base\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Definir el modelo final\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Congelar las capas del modelo base\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    if data_augmentation:\n",
    "        # Aplicar data augmentation durante el entrenamiento\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rotation_range=10,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True\n",
    "        )\n",
    "        train_generator = train_datagen.flow_from_dataframe(\n",
    "            dataframe=train_df,\n",
    "            directory=data_directory,\n",
    "            x_col='Image',\n",
    "            y_col='Category',\n",
    "            target_size=(img_height, img_width),\n",
    "            class_mode='categorical',\n",
    "            batch_size=32\n",
    "        )\n",
    "\n",
    "    if fine_tuning:\n",
    "        # Descongelar las últimas capas del modelo base para el ajuste fino\n",
    "        for layer in base_model.layers[-20:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "    history=model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
    "\n",
    "    try:\n",
    "        model.save(f\"{path}/{name}.h5\")\n",
    "    except:\n",
    "        print(\"Modelo no guardado\")\n",
    "\n",
    "    end = datetime.now()\n",
    "\n",
    "    print(f\"It took {end-start} time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8439bcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "388/388 [==============================] - 577s 1s/step - loss: 0.2259 - accuracy: 0.9307 - val_loss: 0.1610 - val_accuracy: 0.9484\n",
      "Epoch 2/2\n",
      "388/388 [==============================] - 596s 2s/step - loss: 0.0716 - accuracy: 0.9766 - val_loss: 0.1711 - val_accuracy: 0.9562\n",
      "It took 0:19:34.726723 time\n"
     ]
    }
   ],
   "source": [
    "create_model(\".\", \"model_0306_efficientnetB0v2.h5\", 224, 224, train_generator, validation_generator, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6674616",
   "metadata": {},
   "source": [
    "# RESNET50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c0d41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def create_model(path, name, img_height, img_width, train_generator, validation_generator, epochs):\n",
    "    num_classes = 12\n",
    "\n",
    "    start = datetime.now()\n",
    "\n",
    "    # Crear el modelo base\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Definir el modelo final\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Congelar las capas del modelo base\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
    "\n",
    "    try:\n",
    "        model.save(f\"{path}/{name}.h5\")\n",
    "    except:\n",
    "        print(\"Modelo no guardado\")\n",
    "\n",
    "    end = datetime.now()\n",
    "    print(f\"It took {end-start} time\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74dc97c",
   "metadata": {},
   "source": [
    "# PREDICTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57455150",
   "metadata": {},
   "source": [
    "1. Prediction model_0306_efficientnetB0 INITIAL VERSION\n",
    "2. Prediction model_0306_efficientnetB0 TRENAINED\n",
    "3. Prediction model_0306_efficientnetB0 v2 + fine tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88688822",
   "metadata": {},
   "source": [
    "# 1. Prediction model_0306_efficientnetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "160a9071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"model_0306_efficientnetB0.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bf283a",
   "metadata": {},
   "source": [
    "Cardboard prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "894a0062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 799ms/step\n",
      "Confianza (%): 22.29856848716736\n",
      "La imagen es: 8\n",
      "La imagen es: plastic\n",
      "[[0.0977457  0.01822413 0.06794209 0.13780573 0.02394277 0.07599938\n",
      "  0.05021247 0.0256495  0.22298568 0.04322841 0.04679032 0.18947382]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Ruta de la imagen en tu ordenador\n",
    "image_path = r\"C:\\Users\\Leticia Martinez\\Desktop\\IRONHACK\\Labs\\Project_V\\photo_test\\cardboard1.jpg\"\n",
    "\n",
    "# Cargar la imagen en color\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Redimensionar la imagen a las dimensiones requeridas por el modelo\n",
    "img_width, img_height = 224, 224  # Usar las dimensiones adecuadas para tu modelo\n",
    "image = image.resize((img_width, img_height))\n",
    "\n",
    "# Convertir la imagen en un arreglo numpy\n",
    "image_array = np.array(image)\n",
    "\n",
    "# Añadir una dimensión adicional para el batch\n",
    "image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "# Preprocesar la imagen (normalización, etc.) de acuerdo con el modelo\n",
    "image_array = image_array.astype('float32')\n",
    "image_array /= 255\n",
    "\n",
    "predictions = model.predict(image_array)\n",
    "predicted_class = np.argmax(predictions)\n",
    "confidence = np.max(predictions) * 100\n",
    "\n",
    "print(\"Confianza (%):\", confidence)\n",
    "\n",
    "print(\"La imagen es:\", predicted_class)\n",
    "class_labels = ['battery', 'biological', 'brown-glass', 'cardboard', 'clothes', 'green-glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass']\n",
    "\n",
    "predicted_label = class_labels[predicted_class]\n",
    "print('La imagen es:', predicted_label)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b1ad6a",
   "metadata": {},
   "source": [
    "Clothes prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69e1e852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n",
      "Confianza (%): 22.076191008090973\n",
      "La imagen es: 8\n",
      "La imagen es: plastic\n",
      "[[0.0985826  0.01801476 0.06834672 0.13927701 0.02328959 0.07531924\n",
      "  0.05061185 0.02567274 0.22076191 0.04325297 0.04735589 0.18951473]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Ruta de la imagen en tu ordenador\n",
    "image_path = r\"C:\\Users\\Leticia Martinez\\Desktop\\IRONHACK\\Labs\\Project_V\\photo_test\\clothes4.jpg\"\n",
    "\n",
    "# Cargar la imagen en color\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Redimensionar la imagen a las dimensiones requeridas por el modelo\n",
    "img_width, img_height = 224, 224  # Usar las dimensiones adecuadas para tu modelo\n",
    "image = image.resize((img_width, img_height))\n",
    "\n",
    "# Convertir la imagen en un arreglo numpy\n",
    "image_array = np.array(image)\n",
    "\n",
    "# Añadir una dimensión adicional para el batch\n",
    "image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "# Preprocesar la imagen (normalización, etc.) de acuerdo con el modelo\n",
    "image_array = image_array.astype('float32')\n",
    "image_array /= 255\n",
    "\n",
    "predictions = model.predict(image_array)\n",
    "predicted_class = np.argmax(predictions)\n",
    "confidence = np.max(predictions) * 100\n",
    "\n",
    "print(\"Confianza (%):\", confidence)\n",
    "\n",
    "print(\"La imagen es:\", predicted_class)\n",
    "class_labels = ['battery', 'biological', 'brown-glass', 'cardboard', 'clothes', 'green-glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass']\n",
    "\n",
    "predicted_label = class_labels[predicted_class]\n",
    "print('La imagen es:', predicted_label)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133e44f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plastic prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33f1a1a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 51ms/step\n",
      "Confianza (%): 22.18020111322403\n",
      "La imagen es: 8\n",
      "La imagen es: plastic\n",
      "[[0.09863587 0.01780101 0.06843559 0.13816716 0.02324804 0.07571573\n",
      "  0.05064634 0.0255187  0.22180201 0.04312144 0.04701281 0.18989532]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Ruta de la imagen en tu ordenador\n",
    "image_path = r\"C:\\Users\\Leticia Martinez\\Desktop\\IRONHACK\\Labs\\Project_V\\photo_test\\plastic3.jpg\"\n",
    "\n",
    "# Cargar la imagen en color\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Redimensionar la imagen a las dimensiones requeridas por el modelo\n",
    "img_width, img_height = 224, 224  # Usar las dimensiones adecuadas para tu modelo\n",
    "image = image.resize((img_width, img_height))\n",
    "\n",
    "# Convertir la imagen en un arreglo numpy\n",
    "image_array = np.array(image)\n",
    "\n",
    "# Añadir una dimensión adicional para el batch\n",
    "image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "# Preprocesar la imagen (normalización, etc.) de acuerdo con el modelo\n",
    "image_array = image_array.astype('float32')\n",
    "image_array /= 255\n",
    "\n",
    "predictions = model.predict(image_array)\n",
    "predicted_class = np.argmax(predictions)\n",
    "confidence = np.max(predictions) * 100\n",
    "\n",
    "print(\"Confianza (%):\", confidence)\n",
    "\n",
    "print(\"La imagen es:\", predicted_class)\n",
    "class_labels = ['battery', 'biological', 'brown-glass', 'cardboard', 'clothes', 'green-glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass']\n",
    "\n",
    "predicted_label = class_labels[predicted_class]\n",
    "print('La imagen es:', predicted_label)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71985251",
   "metadata": {},
   "source": [
    "# 2. Prediction model_0306_efficientnetB0 RE-TRAINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da66ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model_re = load_model(\"model_0306_efficientnetB0_retrain1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b7301",
   "metadata": {},
   "source": [
    "Cardboard prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8de878c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 54ms/step\n",
      "Confianza (%): 61.84994578361511\n",
      "La imagen es: 8\n",
      "La imagen es: plastic\n",
      "[[3.55317220e-02 7.90315680e-05 2.86725000e-04 1.13074845e-02\n",
      "  1.43049892e-05 1.12471702e-02 2.95381087e-05 2.57319207e-05\n",
      "  6.18499458e-01 1.97430491e-05 3.01908404e-01 2.10507307e-02]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Ruta de la imagen en tu ordenador\n",
    "image_path = r\"C:\\Users\\Leticia Martinez\\Desktop\\IRONHACK\\Labs\\Project_V\\photo_test\\cardboard1.jpg\"\n",
    "\n",
    "# Cargar la imagen en color\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Redimensionar la imagen a las dimensiones requeridas por el modelo\n",
    "img_width, img_height = 224, 224  # Usar las dimensiones adecuadas para tu modelo\n",
    "image = image.resize((img_width, img_height))\n",
    "\n",
    "# Convertir la imagen en un arreglo numpy\n",
    "image_array = np.array(image)\n",
    "\n",
    "# Añadir una dimensión adicional para el batch\n",
    "image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "# Preprocesar la imagen (normalización, etc.) de acuerdo con el modelo\n",
    "image_array = image_array.astype('float32')\n",
    "image_array /= 255\n",
    "\n",
    "predictions = model_re.predict(image_array)\n",
    "predicted_class = np.argmax(predictions)\n",
    "confidence = np.max(predictions) * 100\n",
    "\n",
    "print(\"Confianza (%):\", confidence)\n",
    "\n",
    "print(\"La imagen es:\", predicted_class)\n",
    "class_labels = ['battery', 'biological', 'brown-glass', 'cardboard', 'clothes', 'green-glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass']\n",
    "\n",
    "predicted_label = class_labels[predicted_class]\n",
    "print('La imagen es:', predicted_label)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d838e19",
   "metadata": {},
   "source": [
    "Clothes prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "93cd2266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 53ms/step\n",
      "Confianza (%): 60.5488657951355\n",
      "La imagen es: 8\n",
      "La imagen es: plastic\n",
      "[[3.7629820e-02 7.8805992e-05 3.0195367e-04 1.2075156e-02 1.3901715e-05\n",
      "  1.1129593e-02 3.1575331e-05 2.6693622e-05 6.0548866e-01 2.0411780e-05\n",
      "  3.1134942e-01 2.1853948e-02]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Ruta de la imagen en tu ordenador\n",
    "image_path = r\"C:\\Users\\Leticia Martinez\\Desktop\\IRONHACK\\Labs\\Project_V\\photo_test\\clothes4.jpg\"\n",
    "\n",
    "# Cargar la imagen en color\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Redimensionar la imagen a las dimensiones requeridas por el modelo\n",
    "img_width, img_height = 224, 224  # Usar las dimensiones adecuadas para tu modelo\n",
    "image = image.resize((img_width, img_height))\n",
    "\n",
    "# Convertir la imagen en un arreglo numpy\n",
    "image_array = np.array(image)\n",
    "\n",
    "# Añadir una dimensión adicional para el batch\n",
    "image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "# Preprocesar la imagen (normalización, etc.) de acuerdo con el modelo\n",
    "image_array = image_array.astype('float32')\n",
    "image_array /= 255\n",
    "\n",
    "predictions = model_re.predict(image_array)\n",
    "predicted_class = np.argmax(predictions)\n",
    "confidence = np.max(predictions) * 100\n",
    "\n",
    "print(\"Confianza (%):\", confidence)\n",
    "\n",
    "print(\"La imagen es:\", predicted_class)\n",
    "class_labels = ['battery', 'biological', 'brown-glass', 'cardboard', 'clothes', 'green-glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass']\n",
    "\n",
    "predicted_label = class_labels[predicted_class]\n",
    "print('La imagen es:', predicted_label)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee60b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plastic prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b7830b5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n",
      "Confianza (%): 61.24998927116394\n",
      "La imagen es: 8\n",
      "La imagen es: plastic\n",
      "[[3.7547827e-02 7.5313983e-05 2.9978697e-04 1.1752524e-02 1.3750690e-05\n",
      "  1.1082111e-02 3.1653788e-05 2.6112548e-05 6.1249989e-01 2.0201704e-05\n",
      "  3.0476332e-01 2.1887485e-02]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Ruta de la imagen en tu ordenador\n",
    "image_path = r\"C:\\Users\\Leticia Martinez\\Desktop\\IRONHACK\\Labs\\Project_V\\photo_test\\plastic3.jpg\"\n",
    "\n",
    "# Cargar la imagen en color\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Redimensionar la imagen a las dimensiones requeridas por el modelo\n",
    "img_width, img_height = 224, 224  # Usar las dimensiones adecuadas para tu modelo\n",
    "image = image.resize((img_width, img_height))\n",
    "\n",
    "# Convertir la imagen en un arreglo numpy\n",
    "image_array = np.array(image)\n",
    "\n",
    "# Añadir una dimensión adicional para el batch\n",
    "image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "# Preprocesar la imagen (normalización, etc.) de acuerdo con el modelo\n",
    "image_array = image_array.astype('float32')\n",
    "image_array /= 255\n",
    "\n",
    "predictions = model_re.predict(image_array)\n",
    "predicted_class = np.argmax(predictions)\n",
    "confidence = np.max(predictions) * 100\n",
    "\n",
    "print(\"Confianza (%):\", confidence)\n",
    "\n",
    "print(\"La imagen es:\", predicted_class)\n",
    "class_labels = ['battery', 'biological', 'brown-glass', 'cardboard', 'clothes', 'green-glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass']\n",
    "\n",
    "predicted_label = class_labels[predicted_class]\n",
    "print('La imagen es:', predicted_label)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fe2e69",
   "metadata": {},
   "source": [
    "# 3. Prediction model_0306_efficientnetB0 v2 + fine tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "847918d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"model_0306_efficientnetB0v2.h5.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3f93d8",
   "metadata": {},
   "source": [
    "Cardboard prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ca55649a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 789ms/step\n",
      "Confianza (%): 23.227618634700775\n",
      "La imagen es: 10\n",
      "La imagen es: trash\n",
      "[[0.03479067 0.01096367 0.02473339 0.13694571 0.00771833 0.15003772\n",
      "  0.01777375 0.01272381 0.17304362 0.01811986 0.23227619 0.18087327]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Ruta de la imagen en tu ordenador\n",
    "image_path = r\"C:\\Users\\Leticia Martinez\\Desktop\\IRONHACK\\Labs\\Project_V\\photo_test\\cardboard1.jpg\"\n",
    "\n",
    "# Cargar la imagen en color\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Redimensionar la imagen a las dimensiones requeridas por el modelo\n",
    "img_width, img_height = 224, 224  # Usar las dimensiones adecuadas para tu modelo\n",
    "image = image.resize((img_width, img_height))\n",
    "\n",
    "# Convertir la imagen en un arreglo numpy\n",
    "image_array = np.array(image)\n",
    "\n",
    "# Añadir una dimensión adicional para el batch\n",
    "image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "# Preprocesar la imagen (normalización, etc.) de acuerdo con el modelo\n",
    "image_array = image_array.astype('float32')\n",
    "image_array /= 255\n",
    "\n",
    "predictions = model.predict(image_array)\n",
    "predicted_class = np.argmax(predictions)\n",
    "confidence = np.max(predictions) * 100\n",
    "\n",
    "print(\"Confianza (%):\", confidence)\n",
    "\n",
    "print(\"La imagen es:\", predicted_class)\n",
    "class_labels = ['battery', 'biological', 'brown-glass', 'cardboard', 'clothes', 'green-glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass']\n",
    "\n",
    "predicted_label = class_labels[predicted_class]\n",
    "print('La imagen es:', predicted_label)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b923af80",
   "metadata": {},
   "source": [
    "Clothes prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f488236f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "Confianza (%): 23.16683679819107\n",
      "La imagen es: 10\n",
      "La imagen es: trash\n",
      "[[0.03537419 0.01083573 0.02458145 0.1377377  0.0075525  0.14871559\n",
      "  0.01766006 0.01275216 0.17369632 0.01840163 0.23166837 0.18102431]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Ruta de la imagen en tu ordenador\n",
    "image_path = r\"C:\\Users\\Leticia Martinez\\Desktop\\IRONHACK\\Labs\\Project_V\\photo_test\\clothes4.jpg\"\n",
    "\n",
    "# Cargar la imagen en color\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Redimensionar la imagen a las dimensiones requeridas por el modelo\n",
    "img_width, img_height = 224, 224  # Usar las dimensiones adecuadas para tu modelo\n",
    "image = image.resize((img_width, img_height))\n",
    "\n",
    "# Convertir la imagen en un arreglo numpy\n",
    "image_array = np.array(image)\n",
    "\n",
    "# Añadir una dimensión adicional para el batch\n",
    "image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "# Preprocesar la imagen (normalización, etc.) de acuerdo con el modelo\n",
    "image_array = image_array.astype('float32')\n",
    "image_array /= 255\n",
    "\n",
    "predictions = model.predict(image_array)\n",
    "predicted_class = np.argmax(predictions)\n",
    "confidence = np.max(predictions) * 100\n",
    "\n",
    "print(\"Confianza (%):\", confidence)\n",
    "\n",
    "print(\"La imagen es:\", predicted_class)\n",
    "class_labels = ['battery', 'biological', 'brown-glass', 'cardboard', 'clothes', 'green-glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass']\n",
    "\n",
    "predicted_label = class_labels[predicted_class]\n",
    "print('La imagen es:', predicted_label)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca26f187",
   "metadata": {},
   "source": [
    "Plastisc prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f6bcdb2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 54ms/step\n",
      "Confianza (%): 23.057889938354492\n",
      "La imagen es: 10\n",
      "La imagen es: trash\n",
      "[[0.03521813 0.0106923  0.02449142 0.13761519 0.00754191 0.14961974\n",
      "  0.01771424 0.01274806 0.17468114 0.01835035 0.2305789  0.18074863]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Ruta de la imagen en tu ordenador\n",
    "image_path = r\"C:\\Users\\Leticia Martinez\\Desktop\\IRONHACK\\Labs\\Project_V\\photo_test\\plastic3.jpg\"\n",
    "\n",
    "# Cargar la imagen en color\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Redimensionar la imagen a las dimensiones requeridas por el modelo\n",
    "img_width, img_height = 224, 224  # Usar las dimensiones adecuadas para tu modelo\n",
    "image = image.resize((img_width, img_height))\n",
    "\n",
    "# Convertir la imagen en un arreglo numpy\n",
    "image_array = np.array(image)\n",
    "\n",
    "# Añadir una dimensión adicional para el batch\n",
    "image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "# Preprocesar la imagen (normalización, etc.) de acuerdo con el modelo\n",
    "image_array = image_array.astype('float32')\n",
    "image_array /= 255\n",
    "\n",
    "predictions = model.predict(image_array)\n",
    "predicted_class = np.argmax(predictions)\n",
    "confidence = np.max(predictions) * 100\n",
    "\n",
    "print(\"Confianza (%):\", confidence)\n",
    "\n",
    "print(\"La imagen es:\", predicted_class)\n",
    "class_labels = ['battery', 'biological', 'brown-glass', 'cardboard', 'clothes', 'green-glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass']\n",
    "\n",
    "predicted_label = class_labels[predicted_class]\n",
    "print('La imagen es:', predicted_label)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e5103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras5",
   "language": "python",
   "name": "keras5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
